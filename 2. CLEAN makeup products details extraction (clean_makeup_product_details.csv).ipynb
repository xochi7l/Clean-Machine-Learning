{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1089dbef",
   "metadata": {},
   "source": [
    "I am getting a list of all \"clean\" makeup products from the categories part of the api, such that i can take their product ids and get each clean products details "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc03206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client\n",
    "import json\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c872dca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected details for 376 products.\n"
     ]
    }
   ],
   "source": [
    "# Set up connection\n",
    "conn = http.client.HTTPSConnection(\"sephora14.p.rapidapi.com\")\n",
    "\n",
    "# Define headers w API key\n",
    "headers = {\n",
    "    'x-rapidapi-key': \"ec7e79be26msh4f6ce6532db872cp199109jsn37042832960c\",\n",
    "    'x-rapidapi-host': \"sephora14.p.rapidapi.com\"\n",
    "}\n",
    "\n",
    "# save the detailed product information\n",
    "output_file = 'clean_makeup_product_info.json'\n",
    "\n",
    "# Initialize\n",
    "all_products_details = []\n",
    "page = 1\n",
    "more_products = True\n",
    "\n",
    "while more_products:\n",
    "\n",
    "    endpoint = f\"/searchByCategory?categoryID=clean-makeup&page={page}&sortBy=NEW\"\n",
    "    \n",
    "    try:\n",
    "        # API request\n",
    "        conn.request(\"GET\", endpoint, headers=headers)\n",
    "        res = conn.getresponse()\n",
    "        data = res.read()\n",
    "        decoded_data = data.decode(\"utf-8\")\n",
    "        \n",
    "        # JSON format\n",
    "        products_data = json.loads(decoded_data)\n",
    "        \n",
    "        if 'products' in products_data and len(products_data['products']) > 0:\n",
    "            for product in products_data['products']:\n",
    "                # Extract the details you want\n",
    "                product_details = {\n",
    "                    \"brandName\": product.get('brandName'),\n",
    "                    \"displayName\": product['currentSku'].get('displayName'),\n",
    "                    \"heroImage\": product['currentSku'].get('heroImage'),\n",
    "                    \"altImage\": product['currentSku'].get('altImage'),\n",
    "                    \"onSaleData\": product.get('onSaleData', 'NONE'),\n",
    "                    \"productId\": product.get('productId'),\n",
    "                    \"rating\": product.get('rating'),\n",
    "                    \"reviews\": product.get('reviews'),\n",
    "                    \"targetUrl\": product.get('targetUrl')\n",
    "                }\n",
    "                # add the product details to the list\n",
    "                all_products_details.append(product_details)\n",
    "        else:\n",
    "            # stop the loop\n",
    "            more_products = False\n",
    "        \n",
    "        # next page\n",
    "        page += 1\n",
    "\n",
    "        time.sleep(2)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        more_products = False\n",
    "\n",
    "# all product details to a file\n",
    "with open(output_file, 'w') as file:\n",
    "    json.dump(all_products_details, file, indent=4)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(f\"Collected details for {len(all_products_details)} products.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12927e59",
   "metadata": {},
   "source": [
    "converting the json result from the api to a csv so i can get the product ids of the clean makeup products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d251879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to clean_makeup_product_info.csv\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON data\n",
    "input_file = 'clean_makeup_product_info.json'\n",
    "output_file = 'clean_makeup_product_info.csv'\n",
    "\n",
    "with open(input_file, 'r') as file:\n",
    "    products_data = json.load(file)\n",
    "\n",
    "# CSV headers\n",
    "csv_headers = [\"brandName\", \"displayName\", \"heroImage\", \"altImage\", \"onSaleData\", \"productId\", \"rating\", \"reviews\", \"targetUrl\"]\n",
    "\n",
    "with open(output_file, 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.DictWriter(csvfile, fieldnames=csv_headers)\n",
    "\n",
    "    # Write the headers to the CSV \n",
    "    csvwriter.writeheader()\n",
    "\n",
    "    # Write each product's data to the CSV\n",
    "    for product in products_data:\n",
    "        csvwriter.writerow(product)\n",
    "\n",
    "print(f\"Data has been written to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6298da1e",
   "metadata": {},
   "source": [
    "Getting the clean makup product ids from the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9de7292e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P513383', 'P468685', 'P512014', 'P505233', 'P512320', 'P504041', 'P481997', 'P468207', 'P505440', 'P468175', 'P507492', 'P511709', 'P481403', 'P468694', 'P468206', 'P506279', 'P513229', 'P509577', 'P512778', 'P503741', 'P504609', 'P509340', 'P468436', 'P510360', 'P509334', 'P500283', 'P455936', 'P468693', 'P448854', 'P469825', 'P507142', 'P510885', 'P509439', 'P510823', 'P510654', 'P509414', 'P448852', 'P456151', 'P500123', 'P449342', 'P509844', 'P502185', 'P507110', 'P472997', 'P455418', 'P504025', 'P506671', 'P504152', 'P511117', 'P505327', 'P505655', 'P432668', 'P510741', 'P444439', 'P506281', 'P431750', 'P509397', 'P419107', 'P502484', 'P473327', 'P510426', 'P468210', 'P504884', 'P477829', 'P441813', 'P509440', 'P448853', 'P471048', 'P505832', 'P503996', 'P507751', 'P504907', 'P466123', 'P411848', 'P468696', 'P505462', 'P471049', 'P503903', 'P509720', 'P505694', 'P503895', 'P448845', 'P504318', 'P505330', 'P457455', 'P510999', 'P505541', 'P388953', 'P504893', 'P509047', 'P481999', 'P504183', 'P501795', 'P510731', 'P468864', 'P507750', 'P482000', 'P510442', 'P510438', 'P451920', 'P510072', 'P500284', 'P506794', 'P472978', 'P506013', 'P500448', 'P502170', 'P480189', 'P503943', 'P473304', 'P468692', 'P507322', 'P510159', 'P481396', 'P448855', 'P468691', 'P500314', 'P505433', 'P500480', 'P437097', 'P469819', 'P463071', 'P506382', 'P511696', 'P468455', 'P507471', 'P503938', 'P509390', 'P501589', 'P468697', 'P507338', 'P482002', 'P503732', 'P441873', 'P464772', 'P469839', 'P510570', 'P482001', 'P468204', 'P480188', 'P458281', 'P468653', 'P500336', 'P509899', 'P509873', 'P460814', 'P468383', 'P502850', 'P450514', 'P502461', 'P425737', 'P507753', 'P404797', 'P510877', 'P505757', 'P469873', 'P468429', 'P507343', 'P483685', 'P506921', 'P481998', 'P509434', 'P468384', 'P501568', 'P507794', 'P500301', 'P507320', 'P500043', 'P429548', 'P510895', 'P500321', 'P471047', 'P410543', 'P507664', 'P506182', 'P507099', 'P468431', 'P509342', 'P468432', 'P511109', 'P504293', 'P441883', 'P509345', 'P472807', 'P455324', 'P501811', 'P506360', 'P443764', 'P461456', 'P507783', 'P507996', 'P507768', 'P503754', 'P417170', 'P511937', 'P475966', 'P463072', 'P475110', 'P468430', 'P483470', 'P504604', 'P506812', 'P503896', 'P505617', 'P480579', 'P510474', 'P468382', 'P476713', 'P447744', 'P508880', 'P431758', 'P468385', 'P502321', 'P500329', 'P475128', 'P511747', 'P510655', 'P444438', 'P436270', 'P502288', 'P468380', 'P509657', 'P468863', 'P505699', 'P502178', 'P480618', 'P468435', 'P476011', 'P481996', 'P61003', 'P404798', 'P449098', 'P511748', 'P473842', 'P507104', 'P457828', 'P506022', 'P468211', 'P468433', 'P505422', 'P507070', 'P510037', 'P468434', 'P393356', 'P479309', 'P500298', 'P505738', 'P504886', 'P501215', 'P508431', 'P473848', 'P479308', 'P506725', 'P428673', 'P475111', 'P469847', 'P505634', 'P508752', 'P508701', 'P503922', 'P467765', 'P507345', 'P427652', 'P441874', 'P471245', 'P478523', 'P503762', 'P506563', 'P511394', 'P507166', 'P506000', 'P479835', 'P510726', 'P460512', 'P482557', 'P504905', 'P511650', 'P476712', 'P468444', 'P509877', 'P501217', 'P504222', 'P509872', 'P61004', 'P417912', 'P478730', 'P501569', 'P506785', 'P482055', 'P503767', 'P479643', 'P481384', 'P448152', 'P501572', 'P507754', 'P416774', 'P468445', 'P468443', 'P504127', 'P444920', 'P482699', 'P472317', 'P404793', 'P511745', 'P410176', 'P481819', 'P481128', 'P502814', 'P481088', 'P408286', 'P505874', 'P472179', 'P510888', 'P433124', 'P468442', 'P468441', 'P405290', 'P481129', 'P506005', 'P505952', 'P481828', 'P503763', 'P468438', 'P504874', 'P502965', 'P502770', 'P501177', 'P511731', 'P503357', 'P501155', 'P508415', 'P463098', 'P510722', 'P504322', 'P443307', 'P504330', 'P468440', 'P411508', 'P461132', 'P468439', 'P448856', 'P501444', 'P503759', 'P476892', 'P446824', 'P482053', 'P475143', 'P510518', 'P509845', 'P476014', 'P502289', 'P500425', 'P482730', 'P469546', 'P476005', 'P500333', 'P441884', 'P400017', 'P500294', 'P481160', 'P502066', 'P240604', 'P468381', 'P502081', 'P460779', 'P510363', 'P472178', 'P501780', 'P483464', 'P501766', 'P448846', 'P501578', 'P448847', 'P61006', 'P61005', 'P253810']\n"
     ]
    }
   ],
   "source": [
    "clean_product_ids = df['productId'].tolist()\n",
    "print(clean_product_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a22cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headers w API key\n",
    "headers = {\n",
    "    'x-rapidapi-key': \"ec7e79be26msh4f6ce6532db872cp199109jsn37042832960c\",\n",
    "    'x-rapidapi-host': \"sephora14.p.rapidapi.com\"\n",
    "}\n",
    "\n",
    "# save the output\n",
    "output_file = 'clean_makeup_product_details.json'\n",
    "\n",
    "def get_product_data(clean_product_id):\n",
    "    max_retries = 5  # retries\n",
    "    retries = 0\n",
    "    product_data = {}  # Dictionary to hold product data\n",
    "\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            # Initialize the connection inside the loop\n",
    "            conn = http.client.HTTPSConnection(\"sephora14.p.rapidapi.com\")\n",
    "            \n",
    "            # GET request for the current product ID\n",
    "            conn.request(\"GET\", f\"/product?productID={clean_product_id}\", headers=headers)\n",
    "            res = conn.getresponse()\n",
    "            data = res.read()\n",
    "\n",
    "            # status code\n",
    "            if res.status == 200:\n",
    "                # Store the data for the current product ID\n",
    "                product_data['clean_product_id'] = clean_product_id\n",
    "                product_data['response'] = data.decode(\"utf-8\")\n",
    "                break \n",
    "            elif res.status == 403:\n",
    "                product_data['clean_product_id'] = clean_product_id\n",
    "                product_data['error'] = f\"Failed to retrieve data, Status code: {res.status}\"\n",
    "                break \n",
    "            elif res.status == 429:\n",
    "                print(f\"Rate limit exceeded for product ID: {clean_product_id}. Retrying...\")\n",
    "                time.sleep(10) \n",
    "                retries += 1 \n",
    "            else:\n",
    "                product_data['clean_product_id'] = clean_product_id\n",
    "                product_data['error'] = f\"Failed to retrieve data, Status code: {res.status}\"\n",
    "                break  \n",
    "\n",
    "            conn.close()\n",
    "\n",
    "        except Exception as e:\n",
    "            product_data['clean_product_id'] = clean_product_id\n",
    "            product_data['error'] = f\"An error occurred. Error: {str(e)}\"\n",
    "            retries += 1  \n",
    "\n",
    "    if retries == max_retries:\n",
    "        product_data['clean_product_id'] = clean_product_id\n",
    "        product_data['error'] = \"Max retries exceeded\"\n",
    "\n",
    "    # data to the file\n",
    "    with open(output_file, 'a') as file:\n",
    "        file.write(json.dumps(product_data) + \"\\n\")\n",
    "\n",
    "for clean_product_id in clean_product_ids:\n",
    "    get_product_data(clean_product_id)\n",
    "    time.sleep(2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eca9a9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to clean_makeup_product_details.csv\n"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "all_flattened_data = []\n",
    "\n",
    "# Open the JSON file\n",
    "with open('clean_makeup_product_details.json', 'r') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            product = json.loads(line)\n",
    "            response = json.loads(product['response'])\n",
    "\n",
    "            # Flatten the JSON structure\n",
    "            flattened_data = {\n",
    "                \"Product ID\": product.get(\"clean_product_id\", \"N/A\"),\n",
    "                \"Brand Name\": response.get(\"brand\", {}).get(\"displayName\", \"N/A\"),\n",
    "                \"Product Name\": response.get(\"displayName\", \"N/A\"),\n",
    "                \"List Price\": response.get(\"currentSku\", {}).get(\"listPrice\", \"N/A\"),\n",
    "                \"Size\": response.get(\"currentSku\", {}).get(\"size\", \"N/A\"),\n",
    "                \"Rating\": response.get(\"rating\", \"N/A\"),\n",
    "                \"Number of Reviews\": response.get(\"reviews\", \"N/A\"),\n",
    "                \"Ingredient Description\": response.get(\"currentSku\", {}).get(\"ingredientDesc\", \"N/A\"),\n",
    "                \"Full Product URL\": response.get(\"fullSiteProductUrl\", \"N/A\"),\n",
    "            }\n",
    "\n",
    "            image_urls = response.get(\"currentSku\", {}).get(\"alternateImages\", [])\n",
    "            for idx, image in enumerate(image_urls, start=1):\n",
    "                flattened_data[f\"Image {idx} URL\"] = image.get(\"image1500\", \"N/A\")\n",
    "\n",
    "            # add the flattened data to the list\n",
    "            all_flattened_data.append(flattened_data)\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Skipping line due to JSONDecodeError: {e}\")\n",
    "\n",
    "\n",
    "csv_file = \"clean_makeup_product_details.csv\"\n",
    "\n",
    "keys = set().union(*(d.keys() for d in all_flattened_data))\n",
    "\n",
    "with open(csv_file, mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=keys)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_flattened_data)\n",
    "\n",
    "print(f\"Data successfully written to {csv_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
